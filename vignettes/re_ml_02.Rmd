---
title: "re_ml_02"
author: "Noah Suter"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: yes
---

# AGDS Report

## Report Exercise 6

### Loading all the code from a seperate .R into the markdown

For better datastructure, all the codes that represent no results are in another .R file
```{r warning=FALSE, message=FALSE}
source("../functions/large_files_ex_6.R") #loading all the large code chunks form another .R file
```


### Looking for missing values
I chose to delete the missing values before using the caret function, as the caret function would have removed the NA anyways.
```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Dav_test_1, warn_large_data = FALSE) #checking for missing values in the Davos test set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Dav_train_1, warn_large_data = FALSE) #checking for missing values in the Davos train set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Lae_test_1, warn_large_data = FALSE) #checking for missing values in the Laegern test set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Lae_train_1, warn_large_data = FALSE) #checking for missing values in the Laegern train set
```
There are a lot of missing values in all of the datasets. One can see that there are only NA in P_F for the Laegern datasets. That means that one cannot just delete all the rows containing missing values, as all rows contain missing values. I decided to delete the column P_F. Afterwards i delete all the rows that still contain missing values. As of information of the exercise 9 (one before this exercise), the column LW_IN_F has a lot of missing values (at least within the Davos dataset) and is not decisive for the predicted value. And as the same value should be predicted in this exercise, i decided to delete this column out of all the datasets.


### Within-site validation for Davos models
```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_dav_train_knn, df_train1 = FLX_Dav_train, df_test1 = FLX_Dav_test, mod2 = caret_dav_train_lm, df_train2 = FLX_Dav_train, df_test2 = FLX_Dav_test) #within-site validation for Davos models
```


### Within-site validation for Laegern models
```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_Lae_train_knn, df_train1 = FLX_Lae_train, df_test1 = FLX_Lae_test, mod2 = caret_Lae_train_lm, df_train2 = FLX_Lae_train, df_test2 = FLX_Lae_test) #within-site validation for Laegern models
```

### Across-site validation for Davos models 

```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_dav_train_knn, df_train1 = FLX_Dav_train, df_test1 = FLX_Lae_test, mod2 = caret_dav_train_lm, df_train2 = FLX_Dav_train, df_test2 = FLX_Lae_test) #across-site validation for Davos models
```

### Across-site validation for Laegern models

```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_Lae_train_knn, df_train1 = FLX_Lae_train, df_test1 = FLX_Dav_test, mod2 = caret_Lae_train_lm, df_train2 = FLX_Lae_train, df_test2 = FLX_Dav_test) #across-site validation for the Laegern models
```

### Training a knn model with the pooled dataset
As only one model should be trained, i chose to do the knn model as it is a bit more prone to overfitting and i therefore hope for more generalisable results.



### within site evaluation of the pooled data
```{r warning=FALSE, message=FALSE}
eval_model(mod = caret_combined_train_knn, df_train = bind_FLX_train, df_test = bind_FLX_test) #within site validation for the knn model of the pooled data
```
### Interpretation 

There is a very small difference of the KNN model which indicates that it seems not to be overfitting. Therefore it seems to be well generalizable. If we compare this to the differences found within the across-site validations for the KNN model, it seems as if these are overfitting as there is a big difference to be found. If we compare the values we can see that for the test set, the R2 and the RMSE are suggesting a better model for the pooled data. But it can be a bad idea to pool data together. Following are some reasons why it can be bad.

**Problems with Pooling data from two sites:**
* the sites can have their own unique characterisics, data distributions or patterns.
* by pooling one may intoduce biases or overlook site-specific variations
* there can be site specific nuances that may not be captured by pooling the data

**When could it make sense to pool data together?:**
if both of the sites have similar characteristics, per example: in terms of climate, vegetation, altitude, etc.
Then it could make sense as it could enable a more robust and generalizable model as it uses more data for the training of the model. 
But one would need strong indicaters of that to ensure that the mentioned problems are not influence the model.

the next exercise is a more detailed analysis of the things like the climate, vegetation, altitude, etc of the two sites. This may give an idea whether it made sense to pool the data together.

### Analysis of the characteristics of the two sites


