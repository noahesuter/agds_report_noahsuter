---
title: "re_ml_02"
author: "Noah Suter"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: yes
---

# AGDS Report

## Report Exercise 6

### Loading all the code from a seperate .R into the markdown

For better datastructure, all the codes that represent no results are in another .R file
```{r warning=FALSE, message=FALSE}
source("../functions/large_files_ex_6.R") #loading all the large code chunks form another .R file
```


### Looking for missing values
I chose to delete the missing values before using the caret function, as the caret function would have removed the NA anyways.
```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Dav_test_1, warn_large_data = FALSE) #checking for missing values in the Davos test set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Dav_train_1, warn_large_data = FALSE) #checking for missing values in the Davos train set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Lae_test_1, warn_large_data = FALSE) #checking for missing values in the Laegern test set
```

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(FLX_Lae_train_1, warn_large_data = FALSE) #checking for missing values in the Laegern train set
```
There are a lot of missing values in all of the datasets. One can see that there are only NA in P_F for the Laegern datasets. That means that one cannot just delete all the rows containing missing values, as all rows contain missing values. I decided to delete the column P_F. Afterwards i delete all the rows that still contain missing values. As of information of the exercise 9 (one before this exercise), the column LW_IN_F has a lot of missing values (at least within the Davos dataset) and is not decisive for the predicted value. And as the same value should be predicted in this exercise, i decided to delete this column out of all the datasets.


### Within-site validation for Davos models
```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_dav_train_knn, df_train1 = FLX_Dav_train, df_test1 = FLX_Dav_test, mod2 = caret_dav_train_lm, df_train2 = FLX_Dav_train, df_test2 = FLX_Dav_test) #within-site validation for Davos models
```
#### Interpretation

### Within-site validation for Laegern models
```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_Lae_train_knn, df_train1 = FLX_Lae_train, df_test1 = FLX_Lae_test, mod2 = caret_Lae_train_lm, df_train2 = FLX_Lae_train, df_test2 = FLX_Lae_test) #within-site validation for Laegern models
```
#### Interpretation

### Across-site validation for Davos models 

```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_dav_train_knn, df_train1 = FLX_Dav_train, df_test1 = FLX_Lae_test, mod2 = caret_dav_train_lm, df_train2 = FLX_Dav_train, df_test2 = FLX_Lae_test) #across-site validation for Davos models
```
#### Interpretation

### Across-site validation for Laegern models

```{r warning=FALSE, message=FALSE}
eval_model1(mod1 = caret_Lae_train_knn, df_train1 = FLX_Lae_train, df_test1 = FLX_Dav_test, mod2 = caret_Lae_train_lm, df_train2 = FLX_Lae_train, df_test2 = FLX_Dav_test) #across-site validation for the Laegern models
```
#### Interpretation

### Training a knn model with the pooled dataset
As only one model should be trained, i chose to do the knn model as it is a bit more prone to overfitting and i therefore hope for more generalisable results.



### within site evaluation of the pooled data
```{r warning=FALSE, message=FALSE}
eval_model(mod = caret_combined_train_knn, df_train = bind_FLX_train, df_test = bind_FLX_test) #within site validation for the knn model of the pooled data
```
#### Interpretation


