---
title: "re_stepwise"
author: "Noah Suter"
date: "`r Sys.Date()`"
output: html_document
---
# AGDS Report

## Report Exercise 4

```{r results='hide'}
list.files('../data') #listing the files to find the right data
```

```{r results='hide', message=FALSE}
half_hourly_fluxes <- readr::read_csv("../data/df_for_stepwise_regression.csv") # reading the data into R
```

```{r}
visdat::vis_miss(half_hourly_fluxes) #checking for missing values
```
Due to missing values the R squared values could be influenced and therefore the highest R squared could be wrongly qualified as the highest. This would lead to a wrong decision in the sense of what predictor should be used for the model. But it won't be possible to leave the missing values out as the length of the columns would not match and therefore the modelling process would not be possible.

```{r}
summary(half_hourly_fluxes)
```

here we can see the exact numbers of missing values (NA)



```{r}
half_hourly_fluxes_new_order <- dplyr::select(half_hourly_fluxes,`GPP_NT_VUT_REF`, everything()) #moving our response variable (GPP) column to position one
```

```{r}
mod_summaries <- list() #adding a empty list
```

```{r}
for(i in 2:ncol(half_hourly_fluxes_new_order)) {                 # Head of for-loop
 
  predictors_i <- colnames(half_hourly_fluxes_new_order)[i]    # Create vector of predictor names
  mod_summaries[[i - 1]] <- summary(     # Store regression model summary in list
    lm(GPP_NT_VUT_REF ~ ., half_hourly_fluxes_new_order[ , c("GPP_NT_VUT_REF", predictors_i)]))
 
}
```

```{r results='hide'}
mod_summaries #loading the summaries produced in the junk before
```


```{r}
r_squared_level_1 <- list() #creating a empty list
```


```{r}
for (i in 1:16){
  r_squared_level_1[[i]] <- (mod_summaries[[i]]$r.squared)
} #extracting all the R squared of the bivariate models (single predictor)
```

```{r}
library(base)
highest_index <- which.max(r_squared_level_1)
highest_index #extracting the position of the highest R squared
```
(for position in table +1 must be added. Results in PPFD_IN being the first predicter as it has the highest R squared)
```{r}
mod_summaries[[15]]$r.squared #looking up the highest R squared
```

PPFD_IN as the predictor has the highest R squared. But through the analysis of the data and the missing values we were able to see that there where missing values of 20%. Therefore it can be that the missing values made the R squared to be higher and therefore the model to be of less quality.



```{r warning=FALSE, message=FALSE}
library(ggplot2)

ggplot(half_hourly_fluxes_new_order, aes(x = PPFD_IN , y = GPP_NT_VUT_REF)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5) +
  labs(x = "PPFD_IN", y = "GPP_NT_VUT_REF",
       title = "Regression between GGP and PPFD") #plotting the result of the regression with one predictor
```

Disclaimer: All the rows containing missing values got removed (6137 rows). 
```{r}
half_hourly_fluxes_new_order_2 <- dplyr::select(half_hourly_fluxes_new_order,`GPP_NT_VUT_REF`, `PPFD_IN`, everything()) #moving the first predictor to position 2

```


```{r}
FitStart <- lm(GPP_NT_VUT_REF ~ 1, data = half_hourly_fluxes_new_order_2)
FitStart
```

```{r}
FitAll <- lm(GPP_NT_VUT_REF ~ ., data = half_hourly_fluxes_new_order_2)
FitAll
stats::formula(FitAll)
```
```{r error=TRUE}
stats::step(FitStart, direction = "forward", scope = formula(FitAll))
```

Due to missing values the model does not work. Because of that the number of lines are not the same within all the predictors as the missing values are being removed. Therefore a decision needs to be done. Either the columns with missing values can not be used for modelling. But that could make the model worse as good predicors might not be taken into account (one example would be the first predictor PPFD_IN). Data loss and bias data

Option one: Deleting the missing values (per example with na.omit())
But this is problematic if there are a lot of missing values. As seen by the analysis of the missing data, some columns have a lot of missing values. Therefore this option has the problem of data loss as a result. Due to the high number of missing values this method is not my prefered option.

Option two: Imputing missing values with a reasonable estimate would be the second option. One way would be to do it with the mice package. The problem is that we could be achieving bias data by doing that.

Option three: Model-based imputation would be the third option. But as one would need to build a model for that which could take up a lot of time. DUe to the fact that this option is the most time consuming i prefer option 2 over option 3.

Therefore, I decided to try to impute the missing values by using the mice package and then look at the results using this method. For best results i will try option 1 and option 2


### Option 1 - deleting the rows with the missing values
```{r}
half_hourly_fluxes_op1 <- stats::na.omit(half_hourly_fluxes) #deleting all the rows with missing values
```


```{r}
FitStart_1 <- lm(GPP_NT_VUT_REF ~ 1, data = half_hourly_fluxes_op1)
FitStart
```

```{r}
FitAll_1 <- lm(GPP_NT_VUT_REF ~ ., data = half_hourly_fluxes_op1)
stats::formula(FitAll_1)
```


```{r results='hide'}
stats::step(FitStart_1, direction = "forward", scope = formula(FitAll_1))

```


### Option 2 - Imputing with the mice function

```{r}
str(half_hourly_fluxes)
```
```{r}
mice::md.pattern(half_hourly_fluxes)
```

```{r}
imp_data <- mice::mice(half_hourly_fluxes, m = 57 , maxit = 10, seed = 12345, print = FALSE)
```

```{r}
imp_data
```

```{r}
plot(imp_data)
```

we can see a trend (per example the SF_IN_F_MDS going up, we need to increase the maxit)
```{r}

```



