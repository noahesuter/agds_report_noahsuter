---
title: "re_stepwise_1"
author: "Noah Suter"
date: "`r Sys.Date()`"
output: html_document
---
# AGDS Report

## Report Exercise 4

### Preparing the data


```{r results='hide'}
list.files('../data') #listing the files to find the right data
```

```{r results='hide', message=FALSE}
half_hourly_fluxes <- readr::read_csv("../data/df_for_stepwise_regression.csv") # reading the data into R
```

```{r}
visdat::vis_miss(half_hourly_fluxes) #checking for missing values
```

We can see that some columns do have a lot of missing values. It will be important to keep that in mind for the interpretation of the results and for the creation of the model


```{r results='hide'}
summary(half_hourly_fluxes) #looking at the exact number of missing values (NA) per row
```

```{r}
half_hourly_fluxes_new_order <- dplyr::select(half_hourly_fluxes,`GPP_NT_VUT_REF`, everything()) #moving the column of the response variable (GPP) to position one
```

```{r}
half_hourly_fluxes_clean <- stats::na.omit(half_hourly_fluxes_new_order)
```


## Stepwsise forward regression - bivariate models in a loop

```{r results='hide'}

# Define the response variable
response_var <- "GPP_NT_VUT_REF"

# Define the list to store the results
results <- list()

# Loop through all predictor variables and build bivariate models
for (predictor in c("none", "siteid", "TIMESTAMP", "TA_F", "SW_IN_F", "LW_IN_F", "VPD_F", 
                    "PA_F", "P_F", "WS_F", "TA_F_MDS", "SW_IN_F_MDS", "LW_IN_F_MDS",
                    "VPD_F_MDS", "CO2_F_MDS", "PPFD_IN", "USTAR")) {
  
  if (predictor == "none") {
    # Build the formula string with no predictors
    formula_string <- paste0(response_var, " ~ 1")
  } else {
    # Build the formula string with the current predictor
    formula_string <- paste0(response_var, " ~ ", predictor)
  }
  
  # Create the formula object
  formula <- as.formula(formula_string)
  
  # Fit the linear model
  model <- lm(formula, data = half_hourly_fluxes_clean)
  
  # Store the model results in the list
  results[[predictor]] <- c(formula_string, AIC(model), summary(model)$r.squared, summary(model)$adj.r.squared)
}

# Convert the list of results into a data frame
results_df <- as.data.frame(do.call(rbind, results))

# Rename the columns
colnames(results_df) <- c("formula", "AIC", "r_squared", "adj_r_squared")

# Convert AIC to numeric
results_df$AIC <- as.numeric(results_df$AIC)

# Sort the data frame by AIC value in ascending order
results_df <- results_df[order(results_df$AIC), ]

# Print the results
print(results_df)

```

```{r warning=FALSE, message=FALSE}
library(ggplot2)

ggplot(half_hourly_fluxes_clean, aes(x = PPFD_IN , y = GPP_NT_VUT_REF)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, col="red") +
  labs(x = "PPFD_IN", y = "GPP_NT_VUT_REF",
       title = "Regression between GGP and PPFD") #plotting the result of the regression with one predictor
```

## Full stepwise forward regression with a loop


```{r}
# Define the response variable
response_var <- "GPP_NT_VUT_REF"
# Initialize a vector to store the selected predictors
selected_predictors <- c()
# Initialize a list to store the intermediate models
models <- list()
# Loop through the predictors
for (i in 1:ncol(half_hourly_fluxes_clean)) {
  
  # Initialize a variable to store the best predictor
  best_predictor <- NULL
  
  # Initialize a variable to store the lowest AIC
  lowest_aic <- Inf
  
  # Loop through the remaining predictors
  for (j in setdiff(names(half_hourly_fluxes_clean)[-1], selected_predictors)) {
    
    # Create a formula object for the current predictor
    candidate_formula <- as.formula(paste(response_var, "~", paste(c(selected_predictors, j), collapse = "+")))
    
    # Fit a linear model with the current predictor added
    candidate_lm <- lm(candidate_formula, data = half_hourly_fluxes_clean)
    
    # Check if the AIC is lower than the current lowest AIC
    if (AIC(candidate_lm) < lowest_aic) {
      best_predictor <- j
      lowest_aic <- AIC(candidate_lm)
    }
    
    # Store the intermediate model
    models[[paste(c(selected_predictors, j), collapse = "+")]] <- candidate_lm
  }
  
  # Add the best predictor to the selected predictors
  selected_predictors <- c(selected_predictors, best_predictor)
  
  
}
# Initialize a data frame to store the model statistics
model_stats <- data.frame(Model = character(),
                           AIC = numeric(),
                           Adj_R2 = numeric(),
                           R2 = numeric(),
                           stringsAsFactors = FALSE)
# Loop through the intermediate models and extract the statistics
for (i in seq_along(models)) {
  
  # Extract the model formula and AIC
  model_formula <- formula(models[[i]])
  model_aic <- AIC(models[[i]])
  
  # Extract the model summary and extract the R-squared and adjusted R-squared
  model_summary <- summary(models[[i]])
  model_adj_r2 <- summary(models[[i]])$adj.r.squared
  model_r2 <- summary(models[[i]])$r.squared
  
  # Add the model statistics to the data frame
  model_stats <- rbind(model_stats, data.frame(Model = as.character(model_formula),
                                               AIC = model_aic,
                                               Adj_R2 = model_adj_r2,
                                               R2 = model_r2,
                                               stringsAsFactors = FALSE))
}
```


```{r}
library(dplyr)
library(base)

# Example dataframe

# Group by Formula and summarize by concatenating the values
dataframe_model_values <- model_stats %>%
  group_by(Model) %>%
  summarize(Predictors = paste(Model, collapse = " "),
            AIC = first(AIC),
            Adj_R2= first(Adj_R2),
            R2 = first(R2))
dataframe_model_values[1] <- NULL
```

```{r}
# Sort the model statistics data frame by ascending AIC values
dataframe_model_values <- dataframe_model_values[order(dataframe_model_values$AIC), ]
# Print the top row of the model statistics data frame
cat("Best model based on AIC:\n")
print(dataframe_model_values[1, ])
```


## Full stepwise regression with the function step

```{r results='hide', message=FALSE}
FitStart <- lm(GPP_NT_VUT_REF ~ 1, data = half_hourly_fluxes_clean)
FitStart #Start with an empty model (just intercept)
```

```{r results='hide', message=FALSE}
FitAll <- lm(GPP_NT_VUT_REF ~ ., data = half_hourly_fluxes_clean)
FitAll # creating a model with all the predictors
stats::formula(FitAll) #printing the full formula
```

```{r error=TRUE, warning=FALSE, results='hide'}
stats::step(FitStart, direction = "forward", scope = formula(FitAll)) # doing the stepwise forward regression
```

```{r warning=FALSE}
library(tidyverse)
library(psych)

pairs.panels(half_hourly_fluxes_clean[,c("PPFD_IN", "siteid", "LW_IN_F", "VPD_F_MDS", "TA_F_MDS","SW_IN_F", "WS_F", "USTAR",  "VPD_F", "CO2_F_MDS", "PA_F", "TIMESTAMP", "P_F")], method= "spearman", lm=TRUE)
```




## other optionsto handle the missing data - Imputing the data

As we deleted the missing values, we now have a impact on the quality of the data as almost 2/3 of the rows were deleted. Therefore on could try another option of handling missing data. Another option would be to Impute the data. There one possibility would be to impute the mean, but a better option is to work with mice and run a iterativ process that predicts what the missing values could be based on the values that are not missing. There we can pool the best result and then do a stepwise forward regression with this imputed data. I'm going to show one possibility but will not let the code evaluate as it could take some time to knit that way.$

```{r results='hide'}
mice::md.pattern(half_hourly_fluxes) #checking the pattern of the missing data
```

```{r warning=FALSE, eval=FALSE}
imp_data <- mice::mice(half_hourly_fluxes, m = 57 , maxit = 15, seed = 12345, print = FALSE) #setting the mice options and run the mice function
```

```{r results='hide', eval=FALSE}
imp_data #checking the used methods
```


```{r eval=FALSE}
plot(imp_data) #checking if convergence was achieved
```
afterwards one could run the stepwise forward regression with the imputed data and then compare the outcome with the other model(the one based on the dataframe where the missing values have been deleted). Then one could check which model is better and makes more sense. But as the mice function would need too long to knit i won't to the stepwise forward regression.








